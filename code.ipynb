{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mydata(data.Dataset):\n",
    "    def __init__(self, image_path, label_path, filename, labelname):\n",
    "       \n",
    "        self.root = image_path\n",
    "        df = pd.read_csv(label_path) \n",
    "        \n",
    "        self.image_names  = df[filename].values\n",
    "        self.image_angles = df[labelname].values\n",
    "               \n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_name = os.path.join(self.root, self.image_names[index])\n",
    "        image = Image.open(image_name)\n",
    "        degree = self.image_angles[index]\n",
    "        \n",
    "        random_degree = np.random.randint(-15,15)\n",
    "        image = transforms.functional.rotate(image, random_degree)\n",
    "        degree += random_degree\n",
    "        \n",
    "        pro = transforms.Compose([transforms.ToTensor(),\n",
    "                                         transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.225, 0.225, 0.225])])\n",
    "        image_norm = pro(image)\n",
    "                \n",
    "        return image_norm, degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 16\n",
    "train_set = mydata('Homework_4_files/train',\n",
    "                   'Homework_4_files/train_labels.csv',\n",
    "                   'filename',\n",
    "                   'true_rotation')\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(dataset = train_set , batch_size= batch_size , shuffle = True)\n",
    "\n",
    "valid_set = mydata('Homework_4_files/valid',\n",
    "                   'Homework_4_files/valid_labels.csv',\n",
    "                   'filename',\n",
    "                   'true_rotation')\n",
    "validloader = torch.utils.data.DataLoader(dataset = valid_set , batch_size= batch_size , shuffle = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA IS AVAILABLE!\n"
     ]
    }
   ],
   "source": [
    "run_id ='five'\n",
    "os.mkdir(run_id)\n",
    "criterion=nn.MSELoss(reduction='sum')\n",
    "loss_name='MSE'\n",
    "weight_init='Random'\n",
    "bias_init='Random'\n",
    "learning_rate=0.01\n",
    "num_epoch=100\n",
    "optimizer_name='Adam'\n",
    "\n",
    "# record all hyperparameters that might be useful to reference later\n",
    "with open(run_id + '/hyperparams.csv', 'w') as wfil:\n",
    "    wfil.write(\"weight initialization,\" + weight_init + '\\n')\n",
    "    wfil.write(\"bias initialization,\" + bias_init + '\\n')\n",
    "    wfil.write(\"loss function,\" + loss_name + '\\n')\n",
    "    wfil.write(\"learning rate,\" + str(learning_rate) + '\\n')\n",
    "    wfil.write(\"batch size,\" + str(batch_size) + '\\n')\n",
    "    wfil.write(\"number epochs,\" + str(num_epoch) + '\\n')\n",
    "    wfil.write(\"optimizer,\" + optimizer_name + '\\n')\n",
    "\n",
    "# use resnet-18\n",
    "model = models.resnet18()\n",
    "model.fc = nn.Linear(model.fc.in_features, 1)\n",
    "\n",
    "lis=[{'params':model.layer3.parameters(),'lr':0.0001},\n",
    "     {'params':model.layer4.parameters(),'lr':0.001},\n",
    "     {'params':model.fc.parameters(),'lr':0.001}\n",
    "]\n",
    "optimizer = optim.Adam(lis)\n",
    "scheduler=lr_scheduler.StepLR(optimizer,step_size=10,gamma=0.1)\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    # set the network to use cuda\n",
    "    model = model.cuda()\n",
    "    print(\"CUDA IS AVAILABLE!\")\n",
    "else:\n",
    "    print(\"CUDA NOT AVAILABLE!\")\n",
    "#optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# modify the model for regression, the last layer have 1 neuron for regression\n",
    "# activation is not need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch is 0\n",
      "\n",
      "Training loss is  416.55162954486906\n",
      "Validation loss is  334.5440760117311\n",
      "one epoch takes: 58.93711614608765\n",
      "epoch is 1\n",
      "\n",
      "Training loss is  270.3097634798785\n",
      "Validation loss is  301.25728188455105\n",
      "one epoch takes: 41.86299753189087\n",
      "epoch is 2\n",
      "\n",
      "Training loss is  199.53422446846963\n",
      "Validation loss is  159.61702810127574\n",
      "one epoch takes: 42.11137557029724\n",
      "epoch is 3\n",
      "\n",
      "Training loss is  137.59467604974907\n",
      "Validation loss is  170.32923393409985\n",
      "one epoch takes: 42.27041244506836\n",
      "epoch is 4\n",
      "\n",
      "Training loss is  97.25005077401796\n",
      "Validation loss is  98.94901271343518\n",
      "one epoch takes: 42.25245118141174\n",
      "epoch is 5\n",
      "\n",
      "Training loss is  78.40252230256796\n",
      "Validation loss is  99.66861646192578\n",
      "one epoch takes: 42.43954133987427\n",
      "epoch is 6\n",
      "\n",
      "Training loss is  74.08332165946562\n",
      "Validation loss is  73.24001718025941\n",
      "one epoch takes: 42.21931004524231\n",
      "epoch is 7\n",
      "\n",
      "Training loss is  85.08556317056218\n",
      "Validation loss is  60.96363259959393\n",
      "one epoch takes: 42.368205308914185\n",
      "epoch is 8\n",
      "\n",
      "Training loss is  56.09530665539205\n",
      "Validation loss is  94.31024048448755\n",
      "one epoch takes: 42.456830739974976\n",
      "epoch is 9\n",
      "\n",
      "Training loss is  67.54271824995676\n",
      "Validation loss is  53.711695312307434\n",
      "one epoch takes: 42.46324300765991\n",
      "epoch is 10\n",
      "\n",
      "Training loss is  68.01343772942822\n",
      "Validation loss is  54.188165462504216\n",
      "one epoch takes: 42.10973334312439\n",
      "epoch is 11\n",
      "\n",
      "Training loss is  49.5185403719296\n",
      "Validation loss is  55.17702795880345\n",
      "one epoch takes: 42.07031226158142\n",
      "epoch is 12\n",
      "\n",
      "Training loss is  50.91409090565828\n",
      "Validation loss is  107.78693172407264\n",
      "one epoch takes: 42.048827171325684\n",
      "epoch is 13\n",
      "\n",
      "Training loss is  44.37117544210826\n",
      "Validation loss is  57.08406374775446\n",
      "one epoch takes: 42.1536910533905\n",
      "epoch is 14\n",
      "\n",
      "Training loss is  43.186348470350104\n",
      "Validation loss is  74.52208018087997\n",
      "one epoch takes: 42.0910279750824\n",
      "epoch is 15\n",
      "\n",
      "Training loss is  53.1677209469676\n",
      "Validation loss is  44.31656385241793\n",
      "one epoch takes: 42.12164306640625\n",
      "epoch is 16\n",
      "\n",
      "Training loss is  49.471881424536306\n",
      "Validation loss is  51.651699147831934\n",
      "one epoch takes: 42.26690053939819\n",
      "epoch is 17\n",
      "\n",
      "Training loss is  39.96952505511542\n",
      "Validation loss is  35.56686913394011\n",
      "one epoch takes: 42.373141288757324\n",
      "epoch is 18\n",
      "\n",
      "Training loss is  35.94864461605748\n",
      "Validation loss is  51.26846618319933\n",
      "one epoch takes: 42.289756298065186\n",
      "epoch is 19\n",
      "\n",
      "Training loss is  33.58211963632454\n",
      "Validation loss is  59.08378214532366\n",
      "one epoch takes: 42.02816557884216\n",
      "epoch is 20\n",
      "\n",
      "Training loss is  43.874056951304276\n",
      "Validation loss is  39.85439425921784\n",
      "one epoch takes: 41.96551156044006\n",
      "epoch is 21\n",
      "\n",
      "Training loss is  31.67613277899722\n",
      "Validation loss is  31.258620617490333\n",
      "one epoch takes: 41.96309542655945\n",
      "epoch is 22\n",
      "\n",
      "Training loss is  35.28571224854561\n",
      "Validation loss is  29.615711486970003\n",
      "one epoch takes: 42.002312898635864\n",
      "epoch is 23\n",
      "\n",
      "Training loss is  33.30338444169611\n",
      "Validation loss is  42.79801428432648\n",
      "one epoch takes: 41.99908709526062\n",
      "epoch is 24\n",
      "\n",
      "Training loss is  30.51169614070037\n",
      "Validation loss is  30.99816640003477\n",
      "one epoch takes: 41.97041201591492\n",
      "epoch is 25\n",
      "\n",
      "Training loss is  31.568266267329456\n",
      "Validation loss is  42.34894141970346\n",
      "one epoch takes: 41.989673376083374\n",
      "epoch is 26\n",
      "\n",
      "Training loss is  33.08095410712063\n",
      "Validation loss is  49.590513532837996\n",
      "one epoch takes: 42.05425143241882\n",
      "epoch is 27\n",
      "\n",
      "Training loss is  43.738435083696\n",
      "Validation loss is  28.175981644594756\n",
      "one epoch takes: 42.164753675460815\n",
      "epoch is 28\n",
      "\n",
      "Training loss is  33.67448736171704\n",
      "Validation loss is  31.455614957671898\n",
      "one epoch takes: 42.33420538902283\n",
      "epoch is 29\n",
      "\n",
      "Training loss is  36.896343601134916\n",
      "Validation loss is  45.442557170127444\n",
      "one epoch takes: 42.22514462471008\n",
      "epoch is 30\n",
      "\n",
      "Training loss is  29.370757763981818\n",
      "Validation loss is  47.23519258387387\n",
      "one epoch takes: 42.012481927871704\n",
      "epoch is 31\n",
      "\n",
      "Training loss is  26.781224968768655\n",
      "Validation loss is  27.42584435147448\n",
      "one epoch takes: 41.84129619598389\n",
      "epoch is 32\n",
      "\n",
      "Training loss is  24.331654647386944\n",
      "Validation loss is  25.335199383374018\n",
      "one epoch takes: 41.88725280761719\n",
      "epoch is 33\n",
      "\n",
      "Training loss is  27.62220660872447\n",
      "Validation loss is  34.62731989939661\n",
      "one epoch takes: 41.95202708244324\n",
      "epoch is 34\n",
      "\n",
      "Training loss is  22.221562814613183\n",
      "Validation loss is  34.20471887577038\n",
      "one epoch takes: 41.99511671066284\n",
      "epoch is 35\n",
      "\n",
      "Training loss is  30.021468673224252\n",
      "Validation loss is  33.16106957627031\n",
      "one epoch takes: 42.005791425704956\n",
      "epoch is 36\n",
      "\n",
      "Training loss is  24.26466228865087\n",
      "Validation loss is  27.469405995260995\n",
      "one epoch takes: 42.07675361633301\n",
      "epoch is 37\n",
      "\n",
      "Training loss is  25.06606287792325\n",
      "Validation loss is  32.277887510941724\n",
      "one epoch takes: 42.193609952926636\n",
      "epoch is 38\n",
      "\n",
      "Training loss is  24.137973905863863\n",
      "Validation loss is  29.024402937851846\n",
      "one epoch takes: 42.24317193031311\n",
      "epoch is 39\n",
      "\n",
      "Training loss is  25.306777140696845\n",
      "Validation loss is  30.922237494100745\n",
      "one epoch takes: 42.07306694984436\n",
      "epoch is 40\n",
      "\n",
      "Training loss is  22.48129641378919\n",
      "Validation loss is  42.2430193540282\n",
      "one epoch takes: 41.95182466506958\n",
      "epoch is 41\n",
      "\n",
      "Training loss is  25.91383020358781\n",
      "Validation loss is  26.77896884771494\n",
      "one epoch takes: 41.90412163734436\n",
      "epoch is 42\n",
      "\n",
      "Training loss is  21.18470720337083\n",
      "Validation loss is  37.65118638769938\n",
      "one epoch takes: 42.01380658149719\n",
      "epoch is 43\n",
      "\n",
      "Training loss is  17.937829300450783\n",
      "Validation loss is  28.612727714344285\n",
      "one epoch takes: 42.01902794837952\n",
      "epoch is 44\n",
      "\n",
      "Training loss is  16.30917673394084\n",
      "Validation loss is  32.72973536126889\n",
      "one epoch takes: 41.946526765823364\n",
      "epoch is 45\n",
      "\n",
      "Training loss is  19.652732797053954\n",
      "Validation loss is  25.622784218607613\n",
      "one epoch takes: 41.957125186920166\n",
      "epoch is 46\n",
      "\n",
      "Training loss is  20.119789193846906\n",
      "Validation loss is  26.20443182764575\n",
      "one epoch takes: 41.9821560382843\n",
      "epoch is 47\n",
      "\n",
      "Training loss is  23.629531578719615\n",
      "Validation loss is  23.98226710785485\n",
      "one epoch takes: 42.10057854652405\n",
      "epoch is 48\n",
      "\n",
      "Training loss is  20.475256951649985\n",
      "Validation loss is  29.80667097382964\n",
      "one epoch takes: 42.49989199638367\n",
      "epoch is 49\n",
      "\n",
      "Training loss is  19.865576101007562\n",
      "Validation loss is  24.83844122974775\n",
      "one epoch takes: 42.23473858833313\n",
      "epoch is 50\n",
      "\n",
      "Training loss is  19.892796348979076\n",
      "Validation loss is  23.336805822614295\n",
      "one epoch takes: 41.95420002937317\n",
      "epoch is 51\n",
      "\n",
      "Training loss is  15.275002490112868\n",
      "Validation loss is  23.139506965967414\n",
      "one epoch takes: 41.87284541130066\n",
      "epoch is 52\n",
      "\n",
      "Training loss is  17.69198291792224\n",
      "Validation loss is  21.48255487359487\n",
      "one epoch takes: 41.907639026641846\n",
      "epoch is 53\n",
      "\n",
      "Training loss is  17.412693463686544\n",
      "Validation loss is  26.567408856255216\n",
      "one epoch takes: 41.96356749534607\n",
      "epoch is 54\n",
      "\n",
      "Training loss is  16.49753153647917\n",
      "Validation loss is  22.82137554883957\n",
      "one epoch takes: 41.940757513046265\n",
      "epoch is 55\n",
      "\n",
      "Training loss is  16.208713443769764\n",
      "Validation loss is  35.30222510947631\n",
      "one epoch takes: 42.007431507110596\n",
      "epoch is 56\n",
      "\n",
      "Training loss is  23.229662214095395\n",
      "Validation loss is  25.47231192468182\n",
      "one epoch takes: 42.0932502746582\n",
      "epoch is 57\n",
      "\n",
      "Training loss is  17.481016693115233\n",
      "Validation loss is  32.68176935498531\n",
      "one epoch takes: 42.19238328933716\n",
      "epoch is 58\n",
      "\n",
      "Training loss is  16.106836985598008\n",
      "Validation loss is  23.51955776410894\n",
      "one epoch takes: 42.143813610076904\n",
      "epoch is 59\n",
      "\n",
      "Training loss is  12.76788808508388\n",
      "Validation loss is  28.23930713102723\n",
      "one epoch takes: 41.94289255142212\n",
      "epoch is 60\n",
      "\n",
      "Training loss is  15.428430486059709\n",
      "Validation loss is  28.068584895191286\n",
      "one epoch takes: 41.92666792869568\n",
      "epoch is 61\n",
      "\n",
      "Training loss is  16.319551636129617\n",
      "Validation loss is  27.87217818501477\n",
      "one epoch takes: 41.938530683517456\n",
      "epoch is 62\n",
      "\n",
      "Training loss is  17.485718720753987\n",
      "Validation loss is  23.616912038853535\n",
      "one epoch takes: 41.93964505195618\n",
      "epoch is 63\n",
      "\n",
      "Training loss is  14.166127592176199\n",
      "Validation loss is  23.01290157609261\n",
      "one epoch takes: 41.95783996582031\n",
      "epoch is 64\n",
      "\n",
      "Training loss is  18.312703733357292\n",
      "Validation loss is  33.340697856763235\n",
      "one epoch takes: 41.943896770477295\n",
      "epoch is 65\n",
      "\n",
      "Training loss is  17.27323298609505\n",
      "Validation loss is  37.41050534236889\n",
      "one epoch takes: 42.09624409675598\n",
      "epoch is 66\n",
      "\n",
      "Training loss is  14.432853865350285\n",
      "Validation loss is  20.694426211027\n",
      "one epoch takes: 42.227898597717285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch is 67\n",
      "\n",
      "Training loss is  12.095753305504719\n",
      "Validation loss is  34.58370864376999\n",
      "one epoch takes: 42.07512927055359\n",
      "epoch is 68\n",
      "\n",
      "Training loss is  13.06570172842592\n",
      "Validation loss is  29.77464400117214\n",
      "one epoch takes: 41.900787591934204\n",
      "epoch is 69\n",
      "\n",
      "Training loss is  12.096397295628364\n",
      "Validation loss is  24.280890036804173\n",
      "one epoch takes: 41.87751078605652\n",
      "epoch is 70\n",
      "\n",
      "Training loss is  12.464563234100739\n",
      "Validation loss is  19.896071801821773\n",
      "one epoch takes: 41.9207329750061\n",
      "epoch is 71\n",
      "\n",
      "Training loss is  14.31068656562517\n",
      "Validation loss is  30.291627309929865\n",
      "one epoch takes: 41.99042630195618\n",
      "epoch is 72\n",
      "\n",
      "Training loss is  14.749280537121619\n",
      "Validation loss is  26.70636168222588\n",
      "one epoch takes: 41.924025535583496\n",
      "epoch is 73\n",
      "\n",
      "Training loss is  12.673206864198049\n",
      "Validation loss is  25.62761742951205\n",
      "one epoch takes: 41.865017890930176\n",
      "epoch is 74\n",
      "\n",
      "Training loss is  12.564985313018163\n",
      "Validation loss is  20.742835998821718\n",
      "one epoch takes: 41.87329459190369\n",
      "epoch is 75\n",
      "\n",
      "Training loss is  12.180472302287818\n",
      "Validation loss is  29.106973361367217\n",
      "one epoch takes: 41.961379051208496\n",
      "epoch is 76\n",
      "\n",
      "Training loss is  12.732441749218852\n",
      "Validation loss is  32.37951519111028\n",
      "one epoch takes: 42.02201747894287\n",
      "epoch is 77\n",
      "\n",
      "Training loss is  12.637320802547038\n",
      "Validation loss is  25.61990518294848\n",
      "one epoch takes: 42.16410255432129\n",
      "epoch is 78\n",
      "\n",
      "Training loss is  12.206040353228648\n",
      "Validation loss is  34.13795362255321\n",
      "one epoch takes: 42.00302481651306\n",
      "epoch is 79\n",
      "\n",
      "Training loss is  10.831284280577675\n",
      "Validation loss is  23.618979673152072\n",
      "one epoch takes: 41.89352035522461\n",
      "epoch is 80\n",
      "\n",
      "Training loss is  8.853915645014494\n",
      "Validation loss is  24.38591343544137\n",
      "one epoch takes: 41.87747526168823\n",
      "epoch is 81\n",
      "\n",
      "Training loss is  13.338521928004921\n",
      "Validation loss is  26.443584692539073\n",
      "one epoch takes: 41.830074071884155\n",
      "epoch is 82\n",
      "\n",
      "Training loss is  10.094405078434695\n",
      "Validation loss is  27.355956208533964\n",
      "one epoch takes: 41.89057922363281\n",
      "epoch is 83\n",
      "\n",
      "Training loss is  10.28355495105187\n",
      "Validation loss is  19.741843092040373\n",
      "one epoch takes: 41.921995878219604\n",
      "epoch is 84\n",
      "\n",
      "Training loss is  12.068898268661773\n",
      "Validation loss is  20.685535327388116\n",
      "one epoch takes: 41.967047929763794\n",
      "epoch is 85\n",
      "\n",
      "Training loss is  9.066377977561206\n",
      "Validation loss is  31.753103685493652\n",
      "one epoch takes: 42.119592905044556\n",
      "epoch is 86\n",
      "\n",
      "Training loss is  10.232002227740983\n",
      "Validation loss is  22.68342432551659\n",
      "one epoch takes: 42.027791261672974\n",
      "epoch is 87\n",
      "\n",
      "Training loss is  10.664245626019936\n",
      "Validation loss is  29.48974347544404\n",
      "one epoch takes: 41.856932401657104\n",
      "epoch is 88\n",
      "\n",
      "Training loss is  9.741944739713023\n",
      "Validation loss is  21.10689931989719\n",
      "one epoch takes: 41.88005995750427\n",
      "epoch is 89\n",
      "\n",
      "Training loss is  12.291744709530224\n",
      "Validation loss is  20.304262277956767\n",
      "one epoch takes: 41.82802176475525\n",
      "epoch is 90\n",
      "\n",
      "Training loss is  10.260968555094053\n",
      "Validation loss is  23.240902580320835\n",
      "one epoch takes: 41.92570233345032\n",
      "epoch is 91\n",
      "\n",
      "Training loss is  7.852402485025426\n",
      "Validation loss is  20.741003806774433\n",
      "one epoch takes: 42.15786623954773\n",
      "epoch is 92\n",
      "\n",
      "Training loss is  9.282681089068452\n",
      "Validation loss is  19.38021417477956\n",
      "one epoch takes: 41.81972360610962\n",
      "epoch is 93\n",
      "\n",
      "Training loss is  12.838202795609831\n",
      "Validation loss is  22.671463979264864\n",
      "one epoch takes: 41.874332427978516\n",
      "epoch is 94\n",
      "\n",
      "Training loss is  11.156190072909618\n",
      "Validation loss is  16.263995666773273\n",
      "one epoch takes: 41.93817496299744\n",
      "epoch is 95\n",
      "\n",
      "Training loss is  7.818426469750702\n",
      "Validation loss is  18.517935526199064\n",
      "one epoch takes: 42.00382995605469\n",
      "epoch is 96\n",
      "\n",
      "Training loss is  8.050900629527556\n",
      "Validation loss is  20.798941186629236\n",
      "one epoch takes: 42.12584185600281\n",
      "epoch is 97\n",
      "\n",
      "Training loss is  8.057900023361046\n",
      "Validation loss is  17.770096350389604\n",
      "one epoch takes: 41.92335486412048\n",
      "epoch is 98\n",
      "\n",
      "Training loss is  8.675501018830886\n",
      "Validation loss is  24.12051176623656\n",
      "one epoch takes: 41.73318433761597\n",
      "epoch is 99\n",
      "\n",
      "Training loss is  9.839330803432192\n",
      "Validation loss is  20.620443168167885\n",
      "one epoch takes: 41.66727328300476\n",
      "epoch is 100\n",
      "\n",
      "Training loss is  6.715175002943725\n",
      "Validation loss is  19.02645753807603\n",
      "one epoch takes: 41.7403621673584\n",
      "epoch is 101\n",
      "\n",
      "Training loss is  7.48877051456521\n",
      "Validation loss is  21.631524338064573\n",
      "one epoch takes: 41.84884214401245\n",
      "epoch is 102\n",
      "\n",
      "Training loss is  8.36205243643994\n",
      "Validation loss is  19.45876374367911\n",
      "one epoch takes: 41.85595440864563\n",
      "epoch is 103\n",
      "\n",
      "Training loss is  10.589933917274077\n",
      "Validation loss is  21.045781543791797\n",
      "one epoch takes: 41.76948094367981\n",
      "epoch is 104\n",
      "\n",
      "Training loss is  7.924101749931773\n",
      "Validation loss is  22.797634210580817\n",
      "one epoch takes: 41.81762480735779\n",
      "epoch is 105\n",
      "\n",
      "Training loss is  7.497963820633789\n",
      "Validation loss is  21.081355901291737\n",
      "one epoch takes: 41.9033899307251\n",
      "epoch is 106\n",
      "\n",
      "Training loss is  7.7373953099843735\n",
      "Validation loss is  23.932985350489616\n",
      "one epoch takes: 41.987558364868164\n",
      "epoch is 107\n",
      "\n",
      "Training loss is  7.462979995853578\n",
      "Validation loss is  30.343734575005676\n",
      "one epoch takes: 42.09254503250122\n",
      "epoch is 108\n",
      "\n",
      "Training loss is  7.8917565686410915\n",
      "Validation loss is  24.6025092127518\n",
      "one epoch takes: 41.93620705604553\n",
      "epoch is 109\n",
      "\n",
      "Training loss is  9.053439839348043\n",
      "Validation loss is  19.597017174013533\n",
      "one epoch takes: 41.81561756134033\n",
      "epoch is 110\n",
      "\n",
      "Training loss is  7.389747382290661\n",
      "Validation loss is  24.894254541167847\n",
      "one epoch takes: 41.86966037750244\n",
      "epoch is 111\n",
      "\n",
      "Training loss is  11.236915556428333\n",
      "Validation loss is  21.474429197716884\n",
      "one epoch takes: 41.83119797706604\n",
      "epoch is 112\n",
      "\n",
      "Training loss is  5.47297983061367\n",
      "Validation loss is  24.94880944160888\n",
      "one epoch takes: 41.798938035964966\n",
      "epoch is 113\n",
      "\n",
      "Training loss is  6.608791174804792\n",
      "Validation loss is  28.350155332483926\n",
      "one epoch takes: 41.822019815444946\n",
      "epoch is 114\n",
      "\n",
      "Training loss is  7.046607880899683\n",
      "Validation loss is  22.275379891005848\n",
      "one epoch takes: 41.89275765419006\n",
      "epoch is 115\n",
      "\n",
      "Training loss is  11.394988430712838\n",
      "Validation loss is  20.478181020285074\n",
      "one epoch takes: 42.02753949165344\n",
      "epoch is 116\n",
      "\n",
      "Training loss is  6.640503197250267\n",
      "Validation loss is  20.137375775912705\n",
      "one epoch takes: 42.0605034828186\n",
      "epoch is 117\n",
      "\n",
      "Training loss is  6.158362747977177\n",
      "Validation loss is  16.45229077847818\n",
      "one epoch takes: 41.88254499435425\n",
      "epoch is 118\n",
      "\n",
      "Training loss is  6.3589400954140975\n",
      "Validation loss is  24.581979638395403\n",
      "one epoch takes: 41.76870679855347\n",
      "epoch is 119\n",
      "\n",
      "Training loss is  7.1896974612399935\n",
      "Validation loss is  23.73564012427456\n",
      "one epoch takes: 41.760764360427856\n",
      "epoch is 120\n",
      "\n",
      "Training loss is  7.037759753794719\n",
      "Validation loss is  19.047355021206805\n",
      "one epoch takes: 41.778217792510986\n",
      "epoch is 121\n",
      "\n",
      "Training loss is  6.200352452500956\n",
      "Validation loss is  19.771289011391882\n",
      "one epoch takes: 41.805424451828\n",
      "epoch is 122\n",
      "\n",
      "Training loss is  5.260592818774748\n",
      "Validation loss is  21.812915510283066\n",
      "one epoch takes: 41.8303165435791\n",
      "epoch is 123\n",
      "\n",
      "Training loss is  6.933389453425383\n",
      "Validation loss is  23.009509020699905\n",
      "one epoch takes: 41.9351589679718\n",
      "epoch is 124\n",
      "\n",
      "Training loss is  6.898240947859983\n",
      "Validation loss is  21.018637071005426\n",
      "one epoch takes: 42.03885293006897\n",
      "epoch is 125\n",
      "\n",
      "Training loss is  7.1080975021841\n",
      "Validation loss is  20.59325828575171\n",
      "one epoch takes: 42.013997077941895\n",
      "epoch is 126\n",
      "\n",
      "Training loss is  6.09947615508611\n",
      "Validation loss is  20.681320884623208\n",
      "one epoch takes: 41.77240872383118\n",
      "epoch is 127\n",
      "\n",
      "Training loss is  5.655370172153537\n",
      "Validation loss is  22.326758458422354\n",
      "one epoch takes: 41.765413761138916\n",
      "epoch is 128\n",
      "\n",
      "Training loss is  5.0053569123211\n",
      "Validation loss is  17.7345568063454\n",
      "one epoch takes: 41.816564083099365\n",
      "epoch is 129\n",
      "\n",
      "Training loss is  6.327946915455008\n",
      "Validation loss is  17.27159246032198\n",
      "one epoch takes: 41.84960341453552\n",
      "epoch is 130\n",
      "\n",
      "Training loss is  5.797693241958817\n",
      "Validation loss is  19.670428964381035\n",
      "one epoch takes: 41.774433612823486\n",
      "epoch is 131\n",
      "\n",
      "Training loss is  7.615959541875248\n",
      "Validation loss is  27.016165677458048\n",
      "one epoch takes: 41.77981615066528\n",
      "epoch is 132\n",
      "\n",
      "Training loss is  8.389516303713123\n",
      "Validation loss is  20.637955061410768\n",
      "one epoch takes: 41.802772998809814\n",
      "epoch is 133\n",
      "\n",
      "Training loss is  5.296294606265922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss is  19.488588717694466\n",
      "one epoch takes: 41.94241786003113\n",
      "epoch is 134\n",
      "\n",
      "Training loss is  5.110824024757991\n",
      "Validation loss is  20.887306512763296\n",
      "one epoch takes: 42.344189167022705\n",
      "epoch is 135\n",
      "\n",
      "Training loss is  4.353100437835479\n",
      "Validation loss is  19.27050984478914\n",
      "one epoch takes: 41.973860025405884\n",
      "epoch is 136\n",
      "\n",
      "Training loss is  6.329660395924002\n",
      "Validation loss is  18.003596274970242\n",
      "one epoch takes: 41.83320236206055\n",
      "epoch is 137\n",
      "\n",
      "Training loss is  5.348098498942951\n",
      "Validation loss is  21.991558764129877\n",
      "one epoch takes: 41.80537724494934\n",
      "epoch is 138\n",
      "\n",
      "Training loss is  5.39362276022633\n",
      "Validation loss is  19.583652775752572\n",
      "one epoch takes: 41.77738118171692\n",
      "epoch is 139\n",
      "\n",
      "Training loss is  5.386352292491744\n",
      "Validation loss is  25.116762607430037\n",
      "one epoch takes: 41.90536665916443\n",
      "epoch is 140\n",
      "\n",
      "Training loss is  5.647374367055794\n",
      "Validation loss is  22.927768056710754\n",
      "one epoch takes: 41.906414270401\n",
      "epoch is 141\n",
      "\n",
      "Training loss is  4.6933382532000545\n",
      "Validation loss is  17.533910719534525\n",
      "one epoch takes: 41.8997163772583\n",
      "epoch is 142\n",
      "\n",
      "Training loss is  4.5982467304728925\n",
      "Validation loss is  20.886309581522184\n",
      "one epoch takes: 42.06345081329346\n",
      "epoch is 143\n",
      "\n",
      "Training loss is  6.275615427742402\n",
      "Validation loss is  19.684413435206245\n",
      "one epoch takes: 41.985328912734985\n",
      "epoch is 144\n",
      "\n",
      "Training loss is  4.811248655204351\n",
      "Validation loss is  19.447825352947874\n",
      "one epoch takes: 41.72122240066528\n",
      "epoch is 145\n",
      "\n",
      "Training loss is  4.606995291231821\n",
      "Validation loss is  18.849078686657148\n",
      "one epoch takes: 41.62349343299866\n",
      "epoch is 146\n",
      "\n",
      "Training loss is  5.258205397768567\n",
      "Validation loss is  20.148409402141205\n",
      "one epoch takes: 41.66536903381348\n",
      "epoch is 147\n",
      "\n",
      "Training loss is  6.079231014084459\n",
      "Validation loss is  21.452926840140627\n",
      "one epoch takes: 41.68082308769226\n",
      "epoch is 148\n",
      "\n",
      "Training loss is  6.0171831229740445\n",
      "Validation loss is  24.700281646102667\n",
      "one epoch takes: 41.80427074432373\n",
      "epoch is 149\n",
      "\n",
      "Training loss is  5.388575118267909\n",
      "Validation loss is  18.7113697842265\n",
      "one epoch takes: 41.73470902442932\n",
      "The total time is 105.22500437895457 min\n"
     ]
    }
   ],
   "source": [
    "with open(run_id + '/log_file.csv', 'w') as log_fil:\n",
    "    log_fil.write('epoch,train loss,valid loss\\n')\n",
    "    start = time.time()\n",
    "    \n",
    "    for epoch in range(150):\n",
    "\n",
    "        # track train and validation loss\n",
    "        epoch_train_loss = 0.0\n",
    "        epoch_valid_loss = 0.0\n",
    "        epoch_start = time.time()\n",
    "\n",
    "    \n",
    "        for i, (inputs,labels) in enumerate(trainloader):\n",
    "            # zero out gradients for every batch or they will accumulate\n",
    "            inputs, labels = torch.autograd.Variable(inputs).cuda(), torch.autograd.Variable(labels).cuda()\n",
    "            inputs=inputs.float()\n",
    "            labels=labels.float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward step\n",
    "            outputs = model(inputs)\n",
    "            outputs=outputs.reshape(outputs.shape[0])\n",
    "            labels=labels.reshape(labels.shape[0])\n",
    "\n",
    "            # compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # backwards step\n",
    "            loss.backward()\n",
    "            # update weights and biases\n",
    "            optimizer.step()\n",
    "\n",
    "            # track training loss\n",
    "            epoch_train_loss += loss.item()\n",
    "\n",
    "            # calculate training accuracy\n",
    "        print('epoch is',epoch)\n",
    "        loss = epoch_train_loss/len(trainloader.sampler)\n",
    "        print(\"Training loss is \",loss)\n",
    "    \n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # track valid loss - the torch.no_grad() unsures gradients will not be updated based on validation set\n",
    "        with torch.no_grad():\n",
    "            for i, (inputs,labels) in enumerate(validloader):\n",
    "            # zero out gradients for every batch or they will accumulate\n",
    "                inputs, labels = torch.autograd.Variable(inputs).cuda(), torch.autograd.Variable(labels).cuda()\n",
    "                inputs=inputs.float()\n",
    "                labels=labels.float()\n",
    "                outputs = model(inputs)\n",
    "                outputs=outputs.reshape(outputs.shape[0])\n",
    "                labels=labels.reshape(labels.shape[0])\n",
    "            \n",
    "                valid_loss = criterion(outputs, labels)\n",
    "                epoch_valid_loss += valid_loss.item()\n",
    "            \n",
    "            validloss = epoch_valid_loss/len(validloader.sampler)\n",
    "            print(\"Validation loss is \",validloss)\n",
    "\n",
    "        # save best weights\n",
    "        if best_valid_loss == \"unset\" or validloss < best_valid_loss:\n",
    "            best_valid_loss = validloss\n",
    "            torch.save(model, run_id + \"/best_weights.pth\")\n",
    "        \n",
    "        # save most recent weights\n",
    "        torch.save(model, run_id + \"/last_weights.pth\")\n",
    "        \n",
    "        log_fil.write(str(epoch) + ',' + str(loss) + ',' + str(validloss) + '\\n')\n",
    "        epoch_end = time.time()\n",
    "        print(\"one epoch takes:\", epoch_end - epoch_start)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f\"The total time is {(end-start)/60} min\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = mydata('Homework_4_files/test',\n",
    "                   'Homework_4_files/test_labels.csv',\n",
    "                   'filename',\n",
    "                   'true_rotation')\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(dataset = test_set , batch_size= batch_size , shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss of the model is 530.5353099775315\n"
     ]
    }
   ],
   "source": [
    "weight_fill = \"result/best_weights.pth\"\n",
    "model = torch.load(weight_fil)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    loss = 0\n",
    "    for inputs,labels in testloader:\n",
    "        inputs, labels = torch.autograd.Variable(inputs).cuda(), torch.autograd.Variable(labels).cuda()\n",
    "        inputs=inputs.float()\n",
    "        labels=labels.float()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        outputs=outputs.reshape(outputs.shape[0])\n",
    "        labels=labels.reshape(labels.shape[0])\n",
    "\n",
    "        test_loss = criterion(outputs, labels)\n",
    "        loss += test_loss.item()\n",
    "        \n",
    "        \n",
    "    print('Test loss of the model is',loss/len(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
